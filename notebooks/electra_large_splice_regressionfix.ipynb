{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model_name = \"google/electra-large-discriminator\"\n",
    "# model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Abstract\\mambaforge\\envs\\sentenv2\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, ratings = eval_pred\n",
    "    # predictions = np.argmax(predictions, axis=1)\n",
    "    pred_pos = predictions > 5\n",
    "    label_pos = ratings > 5\n",
    "    # return accuracy.compute(predictions=pred_pos, references=label_pos)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=pred_pos, references=label_pos)[\"accuracy\"],\n",
    "            \"rmse\": np.mean((predictions - ratings)**2)**0.5}\n",
    "\n",
    "# id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "# label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       " }),\n",
       " {'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       "  'label': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"./imdb_reg.py\")\n",
    "imdb.pop(\"unsupervised\")\n",
    "imdb, imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Previous reviewer Claudio Carvalho gave a much better recap of the film's plot details than I could. What I recall mostly is that it was just so beautiful, in every sense - emotionally, visually, editorially - just gorgeous.<br /><br />If you like movies that are wonderful to look at, and also have emotional content to which that beauty is relevant, I think you will be glad to have seen this extraordinary and unusual work of art.<br /><br />On a scale of 1 to 10, I'd give it about an 8.75. The only reason I shy away from 9 is that it is a mood piece. If you are in the mood for a really artistic, very romantic film, then it's a 10. I definitely think it's a must-see, but none of us can be in that mood all the time, so, overall, 8.75.\",\n",
       " 'label': 10.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][12500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=False)\n",
    "    if type(tokens['input_ids'][0]) == list:\n",
    "        for i in range(len(tokens['input_ids'])):\n",
    "            if len(tokens['input_ids'][i]) > 512:\n",
    "                tokens['input_ids'][i] = tokens['input_ids'][i][:129] + \\\n",
    "                    [102] + tokens['input_ids'][i][-382:]\n",
    "                tokens['token_type_ids'][i] = [0]*512\n",
    "                tokens['attention_mask'][i] = [1]*512\n",
    "    elif len(tokens['input_ids']) > 512:\n",
    "        tokens['input_ids'] = tokens['input_ids'][:129] + \\\n",
    "            [102] + tokens['input_ids'][-382:]\n",
    "        tokens['token_type_ids'] = [0]*512\n",
    "        tokens['attention_mask'] = [1]*512\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855b9272e7ff40878f2a0164243d6532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2438fefaf2425bb902b02fb4019aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abstract\\mambaforge\\envs\\sentenv2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d25435cee564949b882753fbdbf8dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 36.9779, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 16.6502, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 13.7007, 'learning_rate': 1.9554168524297815e-05, 'epoch': 0.19}\n",
      "{'loss': 6.0772, 'learning_rate': 1.9108337048595635e-05, 'epoch': 0.26}\n",
      "{'loss': 4.2448, 'learning_rate': 1.8662505572893448e-05, 'epoch': 0.32}\n",
      "{'loss': 5.0142, 'learning_rate': 1.8216674097191264e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3927, 'learning_rate': 1.777084262148908e-05, 'epoch': 0.45}\n",
      "{'loss': 3.4273, 'learning_rate': 1.7325011145786894e-05, 'epoch': 0.51}\n",
      "{'loss': 2.906, 'learning_rate': 1.687917967008471e-05, 'epoch': 0.58}\n",
      "{'loss': 2.6533, 'learning_rate': 1.6433348194382527e-05, 'epoch': 0.64}\n",
      "{'loss': 2.5707, 'learning_rate': 1.598751671868034e-05, 'epoch': 0.7}\n",
      "{'loss': 2.5857, 'learning_rate': 1.5541685242978156e-05, 'epoch': 0.77}\n",
      "{'loss': 2.5658, 'learning_rate': 1.5095853767275971e-05, 'epoch': 0.83}\n",
      "{'loss': 2.7225, 'learning_rate': 1.4650022291573786e-05, 'epoch': 0.9}\n",
      "{'loss': 2.0151, 'learning_rate': 1.42041908158716e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d745676f2b47bc99f30b07fbb76740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.9117867946624756, 'eval_accuracy': 0.88208, 'eval_rmse': 4.946967789532184, 'eval_runtime': 621.0961, 'eval_samples_per_second': 40.251, 'eval_steps_per_second': 2.517, 'epoch': 1.0}\n",
      "{'loss': 2.4671, 'learning_rate': 1.3758359340169416e-05, 'epoch': 1.02}\n",
      "{'loss': 1.8725, 'learning_rate': 1.3312527864467232e-05, 'epoch': 1.09}\n",
      "{'loss': 1.9538, 'learning_rate': 1.2866696388765047e-05, 'epoch': 1.15}\n",
      "{'loss': 1.7274, 'learning_rate': 1.2420864913062862e-05, 'epoch': 1.22}\n",
      "{'loss': 1.9257, 'learning_rate': 1.197503343736068e-05, 'epoch': 1.28}\n",
      "{'loss': 1.8435, 'learning_rate': 1.1529201961658493e-05, 'epoch': 1.34}\n",
      "{'loss': 1.6858, 'learning_rate': 1.1083370485956308e-05, 'epoch': 1.41}\n",
      "{'loss': 1.9954, 'learning_rate': 1.0637539010254126e-05, 'epoch': 1.47}\n",
      "{'loss': 1.7033, 'learning_rate': 1.019170753455194e-05, 'epoch': 1.54}\n",
      "{'loss': 1.4969, 'learning_rate': 9.745876058849756e-06, 'epoch': 1.6}\n",
      "{'loss': 1.5146, 'learning_rate': 9.30004458314757e-06, 'epoch': 1.66}\n",
      "{'loss': 2.0415, 'learning_rate': 8.854213107445387e-06, 'epoch': 1.73}\n",
      "{'loss': 2.0009, 'learning_rate': 8.408381631743202e-06, 'epoch': 1.79}\n",
      "{'loss': 1.7283, 'learning_rate': 7.962550156041017e-06, 'epoch': 1.86}\n",
      "{'loss': 1.7819, 'learning_rate': 7.516718680338832e-06, 'epoch': 1.92}\n",
      "{'loss': 1.5547, 'learning_rate': 7.070887204636649e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9f2bfbdc145bd8e21787d7d86c98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6247199773788452, 'eval_accuracy': 0.95872, 'eval_rmse': 4.865195563248444, 'eval_runtime': 619.8743, 'eval_samples_per_second': 40.331, 'eval_steps_per_second': 2.521, 'epoch': 2.0}\n",
      "{'loss': 1.4602, 'learning_rate': 6.625055728934463e-06, 'epoch': 2.05}\n",
      "{'loss': 1.2891, 'learning_rate': 6.179224253232279e-06, 'epoch': 2.11}\n",
      "{'loss': 1.2627, 'learning_rate': 5.733392777530095e-06, 'epoch': 2.18}\n",
      "{'loss': 1.4664, 'learning_rate': 5.28756130182791e-06, 'epoch': 2.24}\n",
      "{'loss': 1.2305, 'learning_rate': 4.841729826125725e-06, 'epoch': 2.3}\n",
      "{'loss': 1.2872, 'learning_rate': 4.39589835042354e-06, 'epoch': 2.37}\n",
      "{'loss': 1.2644, 'learning_rate': 3.950066874721356e-06, 'epoch': 2.43}\n",
      "{'loss': 1.3754, 'learning_rate': 3.5042353990191713e-06, 'epoch': 2.5}\n",
      "{'loss': 1.1153, 'learning_rate': 3.0584039233169866e-06, 'epoch': 2.56}\n",
      "{'loss': 1.3498, 'learning_rate': 2.612572447614802e-06, 'epoch': 2.62}\n",
      "{'loss': 1.2545, 'learning_rate': 2.1667409719126175e-06, 'epoch': 2.69}\n",
      "{'loss': 1.0703, 'learning_rate': 1.7209094962104325e-06, 'epoch': 2.75}\n",
      "{'loss': 1.1519, 'learning_rate': 1.2750780205082481e-06, 'epoch': 2.82}\n",
      "{'loss': 1.0694, 'learning_rate': 8.292465448060634e-07, 'epoch': 2.88}\n",
      "{'loss': 0.9306, 'learning_rate': 3.834150691038788e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a732abdcb7a49839281c68af70ad9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7369074821472168, 'eval_accuracy': 0.96212, 'eval_rmse': 5.055060677999024, 'eval_runtime': 635.5792, 'eval_samples_per_second': 39.334, 'eval_steps_per_second': 2.459, 'epoch': 3.0}\n",
      "{'train_runtime': 7900.3535, 'train_samples_per_second': 9.493, 'train_steps_per_second': 0.297, 'train_loss': 3.3375680559575938, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2343, training_loss=3.3375680559575938, metrics={'train_runtime': 7900.3535, 'train_samples_per_second': 9.493, 'train_steps_per_second': 0.297, 'train_loss': 3.3375680559575938, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"electra_large_imdb_reg_spliced_fix10\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,\n",
    "    # torch_compile=True,\n",
    "    # fp16=True,\n",
    "    # load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[9.1055]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**(tokenizer(\"\"\"When I was a kid I watched this many times over, and I remember whistling the \"Happy Cat\" song quite often. All the songs are great, and actually memorable, unlike many children's musicals, where the songs are just stuck in for no real reason. The scenes and costumes are lavish, and the acting is very well-done, which isn't surprising, considering the cast. Christopher Walken is very catlike, and doesn't need stupid make-up, or a cat costume for the viewer to believe he's a cat transformed to a human. And Jason Connery's so cute, as the shy and awkward miller's son, Corin, who falls in love with beautiful and the bold Princess Vera. This is a really fun, enjoyable, feature-length movie, where unlike most fairytales, the characters are given personalities. Some of my favourite parts are when Puss makes Corin pretend he's drowning; at the ball when everybody starts dancing a country dance, as it's \"all the rage abroad\"; when Walken is in the kitchen, dancing on the table (he's a pretty good dancer, too!); and when Vera tells Corin all the things she used to do when she was young, like pretending she was a miller's daughter. I'd recommend this film to children and parents alike, who love magic and fairytales. And it actually IS a movie you can watch together, as it won't drive adults up the wall.\"\"\", return_tensors=\"pt\").to('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.9524]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**(tokenizer(\"It was a bit boring. Kinda drawn out but fine ig.\", return_tensors=\"pt\").to('cuda'))).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
